{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Detect AI Generated（bert）\n",
    "[Link](https://www.kaggle.com/code/sunshine888888/llm-detect-ai-generated-bert/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data, Preprocessing\n",
    "train = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\n",
    "test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n",
    "train_len = len(train) \n",
    "\n",
    "all_data = pd.concat([train,test]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv')\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['generated'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv')\n",
    "new_data.head()\n",
    "#在引入的数据库上进行训练,样本数比较多,label分布比较均衡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.prompt_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在该数据集中筛选题目给定的promptname\n",
    "title = prompts['prompt_name']\n",
    "new_train = new_data.iloc[:,0:2][new_data.prompt_name.isin(title)]\n",
    "new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重新构建训练集\n",
    "train_data = new_train.reset_index(drop=True)#修改索引值\n",
    "train_data #0表示学生写的，1表示机器生成的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#绘制生成与否分布直方图\n",
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots()\n",
    "distribution = train_data.label.value_counts()\n",
    "\n",
    "distribution.plot(kind='bar',label='distribution of generated',alpha=.65)\n",
    "# 添加数据标签\n",
    "for i, count in enumerate(distribution):\n",
    "    ax.text(i, count, str(count), ha='center', va='bottom')# ha参数控制水平对齐方式, va控制垂直对齐方式\n",
    "ax.set_xticklabels([\"ungenerated\",\"generated\"], rotation=0)\n",
    "ax.set_ylabel('count')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Cleaning Functions\n",
    "def remove_tag(text):\n",
    "    tag = re.compile(r'@\\S+')#匹配@之后的连续字符，`\\S`匹配任何非空白字符（相当于 `[^ \\t\\n\\r\\f\\v]`），`+` 表示前面的模式可以重复一次或多次。\n",
    "    return tag.sub(r'',text)#使用sub函数用空串替换\n",
    "\n",
    "def remove_URL(text):\n",
    "    # http:... / https:... / www... #匹配网页链接\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return re.sub(url,'',text)\n",
    "\n",
    "def remove_html(text):#匹配特殊符号\n",
    "    # < > / ( )\n",
    "    html = re.compile(r'<[^>]+>|\\([^)]+\\)')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_punct(text):#替换标点符号\n",
    "    # ['!','\"','$','%','&',\"'\",'(',')','*',\n",
    "    # '+',',','-','.','/',':',';','<','=',\n",
    "    # '>','?','@','[','\\\\',']','^','_','`',\n",
    "    # '{','|','}','~']\n",
    "    punctuations = list(string.punctuation)\n",
    "    table = str.maketrans('', '', ''.join(punctuations))#maketrans空字符串替换标点符号\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#找到停用词和标点符号\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.tokenize import word_tokenize #该函数用于将文本分割成单词（tokens）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['cleaned'] = train_data['text'].apply(lambda x:remove_tag(x))\n",
    "train_data['cleaned'] = train_data['cleaned'].apply(lambda x: remove_URL(x))\n",
    "train_data['cleaned'] = train_data['cleaned'].apply(lambda x: remove_html(x))\n",
    "train_data['cleaned'] = train_data['cleaned'].apply(lambda x: remove_punct(x))\n",
    "train_data['cleaned'] = train_data['cleaned'].apply(lambda x: x.lower()) # lowering全部变成小写字母\n",
    "train_data['cleaned'] = train_data['cleaned'].apply(lambda x: word_tokenize(x)) # split sentence into words list\n",
    "# exclude stop words and make them a sentence again把停用词移除并连成一个句子\n",
    "train_data['cleaned'] = train_data['cleaned'].apply(lambda x: ' '.join([word for word in x if word not in stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def get_top_ngram(corpus, n=None):#统计2元高频词元出现次数\n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)#这两步合并为fit_transform函数，将文本中的词语转换为词频矩阵,矩阵元素a[i][j]表示j词在第i个文本下的词频，即各个词语出现的次数,\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx])\n",
    "                  for word, idx in vec.vocabulary_.items()] #vocabulary_属性是一个字典，键是词语，值是词频矩阵的列索引，sum_words[0, idx]表示的是word的词频\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AI-generated\n",
    "top_n_bigrams=get_top_ngram(train_data[train_data['label'] == 1]['cleaned'],2)[:10] #2gram\n",
    "x,y=map(list,zip(*top_n_bigrams))#（*）操作符用于解包上述生成的元组，map(funciton,iterable..)将function应用于iterable的元素，并返回一个迭代器\n",
    "plt.barh(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud for AI-Generated Text\n",
    "from wordcloud import WordCloud\n",
    "ai_generated_text = \" \".join(train_data[train_data['label'] == 1]['cleaned'])\n",
    "wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white').generate(ai_generated_text)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')#imshow在图像中显示数组数据,采用双线性插值\n",
    "plt.title('Word Cloud for AI-Generated Text')\n",
    "plt.axis('off')#Turn off axis lines and labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Student\n",
    "top_n_bigrams=get_top_ngram(train_data[train_data['label'] == 0]['cleaned'],2)[:10] #2gram\n",
    "x,y=map(list,zip(*top_n_bigrams)) \n",
    "plt.barh(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud for student Text\n",
    "from wordcloud import WordCloud\n",
    "ai_generated_text = \" \".join(train_data[train_data['label'] == 0]['cleaned'])\n",
    "wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white').generate(ai_generated_text)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Word Cloud for AI-Generated Text')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "model_name = 'bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)#加载特定的预训练模型tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "#定义数据集\n",
    "class LLMDataset(Dataset):\n",
    "    def __init__(self,df,is_grad,tokenizer):\n",
    "        self.df = df # Pandas.DataFrame\n",
    "        self.is_grad = is_grad # True: train,valid / False: test\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) # number of samples\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        text = self.df.loc[idx,'cleaned'] # extracting text from each row\n",
    "        \n",
    "        encoded_dict = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,#自动在每个文本前后添加特殊标记(如CLS和SEP)\n",
    "            padding='max_length',#补0\n",
    "            truncation=True,#句子长度大于max_length时截断\n",
    "            max_length=512, # given to the max_length of tokenized text\n",
    "            return_tensors='pt', # PyTorch\n",
    "            return_attention_mask=True, # We should put it into the model，计算注意力（attention）时忽略那些paddle值\n",
    "        )\n",
    "\n",
    "        if self.is_grad:#训练集\n",
    "            labels = self.df.loc[idx]['label']\n",
    "            # [batch,1,max_len(84)] -> [batch,max_len]#使用squeeze降维\n",
    "            return {'input_ids':encoded_dict['input_ids'].squeeze(),\n",
    "                    'attention_mask':encoded_dict['attention_mask'].squeeze(),\n",
    "                    'token_type_ids':encoded_dict['token_type_ids'].squeeze(),\n",
    "                   # Our loss_fn wants it to be a \"float\" type\n",
    "                    'labels':torch.tensor(labels,dtype=torch.float).unsqueeze(dim=0)}\n",
    "        else:#测试集\n",
    "            # [batch,1,max_len(84)] -> [batch,max_len]\n",
    "            return {'input_ids':encoded_dict['input_ids'].squeeze(),\n",
    "                    'attention_mask':encoded_dict['attention_mask'].squeeze(),\n",
    "                   'token_type_ids':encoded_dict['token_type_ids'].squeeze()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_data #方便后续train不动进行特征工程，不改动新建好的原始数据train_data\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对测试集数据进行文本预处理\n",
    "test['cleaned'] = test['text'].apply(lambda x:remove_tag(x))\n",
    "test['cleaned'] = test['cleaned'].apply(lambda x: remove_URL(x))\n",
    "test['cleaned'] = test['cleaned'].apply(lambda x: remove_html(x))\n",
    "test['cleaned'] = test['cleaned'].apply(lambda x: remove_punct(x))\n",
    "test['cleaned'] = test['cleaned'].apply(lambda x: x.lower()) # lowering全部变成小写字母\n",
    "test['cleaned'] = test['cleaned'].apply(lambda x: word_tokenize(x)) # split sentence into words list\n",
    "# exclude stop words and make them a sentence again把停用词移除并连成一个句子\n",
    "test['cleaned'] = test['cleaned'].apply(lambda x: ' '.join([word for word in x if word not in stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LLMDataset(train_df,True,tokenizer)\n",
    "test_dataset = LLMDataset(test,False,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset)) # train:valid = 8:2\n",
    "valid_size = len(train_dataset) - train_size\n",
    "\n",
    "train_dataset,valid_dataset = random_split(train_dataset,[train_size,valid_size])\n",
    "print(f'{len(train_dataset)} train samples')\n",
    "print(f'{len(valid_dataset)} valid samples')\n",
    "print(f'{len(test_dataset)} test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=8,shuffle=True,pin_memory=True)#锁页内存(pin_memory)能够保持与GPU进行高速传输,在训练时加快数据的读取\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=8,shuffle=False,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成测试集\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=1,shuffle=True,pin_memory=True)#锁页内存(pin_memory)能够保持与GPU进行高速传输,在训练时加快数据的读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_eval_dataset = LLMDataset(train_df[train_size:].reset_index(drop=False),False,tokenizer)\n",
    "valid_eval_dataloader = DataLoader(valid_eval_dataset,batch_size=1,shuffle=False,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'model_name':'bert-large-uncased',\n",
    "    'num_labels':2,\n",
    "    'batch_size':8,\n",
    "    'epochs':4,\n",
    "    'learning_rate':5e-6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Never Detach Tensor during forward\n",
    "class LLMModel(nn.Module):\n",
    "    '''\n",
    "    To be honest, under the setting like this, there is no need to inherit.\n",
    "    It's because I used \"BertForSequenceClassification\" which has final layer\n",
    "    that is composed of \"hidden size 2\" for binary classification.\n",
    "\n",
    "    So, you can think of this unnecessary inheritance is kind of \"practice\" for myself :)\n",
    "    '''    \n",
    "    def __init__(self,model_name):\n",
    "        super().__init__()\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name)#from_pretrained方法会加载相应的预训练模型权重\n",
    "\n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        output = self.model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        logits = output.logits\n",
    "        return logits\n",
    "    #`logits` 是预训练bert的输出经过线性层的输出，通常用于二分类或多分类任务，通过softmax转换为概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#检查gpu配置\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print('GPU is running on..')\n",
    "else: \n",
    "    device = 'cpu'\n",
    "    print('CPU is running on..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLMModel(configs['model_name']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                lr=6e-6,\n",
    "                eps=1e-8,\n",
    "                no_deprecation_warning=True)\n",
    "\n",
    "# metric for validation\n",
    "# f1_score(y_label,y_pred)\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "metric = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc,os\n",
    "from tqdm.auto import tqdm # visualizing tool for progress\n",
    "\n",
    "# They will be used to pick the best model.pt given to the valid loss\n",
    "best_model_epoch, valid_loss_values = [],[] \n",
    "valid_loss_min = [1] # arbitrary loss I set here\n",
    "def train(model,device,train_dataloader,valid_dataloader,epochs,loss_fn,optimizer,metric):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        gc.collect() # memory cleaning垃圾回收机制，减少占用内存\n",
    "        model.train()\n",
    "\n",
    "        train_loss = 0\n",
    "        train_step = 0\n",
    "        pbar = tqdm(train_dataloader)#tqdm参数是一个iterable\n",
    "\n",
    "        for batch in pbar: # you can also write like \"for batch in tqdm(train_dataloader\"\n",
    "            optimizer.zero_grad() # initialize\n",
    "            train_step += 1\n",
    "\n",
    "            train_input_ids = batch['input_ids'].to(device)\n",
    "            train_attention_mask = batch['attention_mask'].to(device)\n",
    "            train_labels = batch['labels'].squeeze().to(device).long()#long()转化成一维张量\n",
    "            \n",
    "            # You can refer to the class \"TweetsModel\" for understand \n",
    "            # what would be logits\n",
    "            logits = model(train_input_ids, train_attention_mask).to(device)\n",
    "            predictions = torch.argmax(logits, dim=1) # get an index from larger one\n",
    "            detached_predictions = predictions.detach().cpu().numpy()\n",
    "            \n",
    "            loss = loss_fn(logits, train_labels)\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "            train_loss += loss.detach().cpu().numpy().item()\n",
    "\n",
    "            pbar.set_postfix({'train_loss':train_loss/train_step})#设置进度条显示信息\n",
    "        pbar.close()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            valid_loss = 0\n",
    "            valid_step = 0\n",
    "            total_valid_score = 0\n",
    "\n",
    "            y_pred = [] # for getting f1_score that is a metric of the competition\n",
    "            y_true = []\n",
    "\n",
    "            pbar = tqdm(valid_dataloader)\n",
    "            for batch in pbar:\n",
    "                valid_step += 1\n",
    "\n",
    "                valid_input_ids = batch['input_ids'].to(device)\n",
    "                valid_attention_mask = batch['attention_mask'].to(device)\n",
    "                valid_labels = batch['labels'].squeeze().to(device).long()\n",
    "\n",
    "                logits = model(valid_input_ids, valid_attention_mask).to(device)\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                detached_predictions = predictions.detach().cpu().numpy()\n",
    "                \n",
    "                loss = loss_fn(logits, valid_labels)\n",
    "                valid_loss += loss.detach().cpu().numpy().item()\n",
    "\n",
    "                y_pred.extend(predictions.cpu().numpy())\n",
    "                y_true.extend(valid_labels.cpu().numpy())\n",
    "\n",
    "            valid_loss /= valid_step\n",
    "            f1 = f1_score(y_true,y_pred)\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{epochs}] Score: {f1}')\n",
    "            print(f'Epoch [{epoch+1}/{epochs}] Valid_loss: {valid_loss}')\n",
    "\n",
    "            if valid_loss < min(valid_loss_min):\n",
    "                print('model improved!')\n",
    "            else:\n",
    "                print('model not improved')\n",
    "    \n",
    "            torch.save(model.state_dict(), f'epoch:{epoch+1}_model.pt')#state_dict 是一个字典对象，包含了模型的所有可学习参数（如权重和偏置）及其当前值\n",
    "            print('save checkpoint!')\n",
    "            valid_loss_min.append(valid_loss)\n",
    "            print(f'valid_loss_min:{min(valid_loss_min)}')\n",
    "\n",
    "        best_model_epoch.append(f'/kaggle/working/epoch:{epoch+1}_model.pt')\n",
    "        valid_loss_values.append(valid_loss)\n",
    "        print('='*100)\n",
    "\n",
    "    select_best_model() # refer to below function\n",
    "    print('Train/Valid Completed!!')\n",
    "    del train_dataloader, valid_dataloader # memory cleaning\n",
    "    gc.collect()\n",
    "\n",
    "def select_best_model():\n",
    "    best_model = best_model_epoch[np.array(valid_loss_values).argmin()]\n",
    "    os.rename(best_model, best_model.split('.pt')[0] + '_best.pt')#重命名文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Before training, files in current directory: {os.listdir()}')#列出当前工作目录下的所有文件和子目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Start!')\n",
    "print('=' * 100)\n",
    "\n",
    "train(model,\n",
    "    device,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    configs['epochs'],\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    metric)\n",
    "\n",
    "del model,train_dataloader, valid_dataloader\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model,test_dataloader):\n",
    "    all_preds = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            \n",
    "            logits = model(input_ids,attention_mask)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            all_preds.append(logits)\n",
    "    \n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir():\n",
    "    if 'best.pt' in filename: \n",
    "        best_pt = filename\n",
    "print(f'Best model.pt: {best_pt}')\n",
    "check_point = torch.load(best_pt)\n",
    "\n",
    "# We have to load a model again because I deleted after training/validation\n",
    "model = LLMModel(configs['model_name']).to(device)\n",
    "model.to(device)\n",
    "model.load_state_dict(check_point)#将之前保存的模型参数加载到当前模型的实例中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_valid = inference(model,valid_eval_dataloader)#验证集预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_valid = np.argmax(pre_valid,axis=2)\n",
    "y_valid = train_df.label[train_size:].reset_index(drop=True)#验证集真实值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#绘制ROC曲线，计算AUC面积\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, roc_curve, auc, ConfusionMatrixDisplay  # 导入一些评估指标和绘图函数\n",
    "\n",
    "\n",
    "# 计算ROC曲线的参数\n",
    "fpr, tpr, thresholds = roc_curve(y_valid, pre_valid)  # 计算真阳性率、假阳性率和阈值\n",
    "roc_auc = auc(fpr, tpr)  # 计算AUC值\n",
    "print(roc_auc)\n",
    "\n",
    "# 绘制ROC曲线\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)  # 绘制ROC曲线\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # 绘制随机猜测曲线\n",
    "plt.xlim([0.0, 1.0])  # 设置x轴的取值范围\n",
    "plt.ylim([0.0, 1.05])  # 设置y轴的取值范围\n",
    "plt.xlabel('False Positive Rate')  # 设置x轴标签\n",
    "plt.ylabel('True Positive Rate')  # 设置y轴标签\n",
    "plt.title('Receiver operating characteristic example')  # 设置图表标题\n",
    "plt.legend(loc=\"lower right\")  # 添加图例\n",
    "plt.grid(color='purple', linestyle='--')  # 添加网格线\n",
    "plt.show()  # 显示图表\n",
    "\n",
    "# 绘制混淆矩阵\n",
    "ConfusionMatrixDisplay.from_predictions(y_valid,pre_valid, colorbar=True, display_labels=[\"0\", \"1\"],\n",
    "                                         cmap=plt.cm.Reds)  # 根据预测结果和真实结果绘制混淆矩阵\n",
    "plt.title(\"Confusion Matrix\")  # 设置图表标题\n",
    "plt.show()  # 显示图表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick up the model.pt written with the best\n",
    "# which has the lowest validation loss through all Epochs.\n",
    "\n",
    "for filename in os.listdir():\n",
    "    if 'best.pt' in filename: \n",
    "        best_pt = filename\n",
    "print(f'Best model.pt: {best_pt}')\n",
    "check_point = torch.load(best_pt)\n",
    "\n",
    "# We have to load a model again because I deleted after training/validation\n",
    "model = LLMModel(configs['model_name']).to(device)\n",
    "model.to(device)\n",
    "model.load_state_dict(check_point)#将之前保存的模型参数加载到当前模型的实例中\n",
    "\n",
    "predictions = inference(model,test_dataloader) #predictions形状为(batch_size,num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "pre_test = F.softmax(torch.tensor(predictions),dtype=torch.float32,dim=-1) #logits经过softmax之后的概率\n",
    "prob_test = pre_test[:,0,1] #generated的概率\n",
    "prob_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['generated'] = prob_test\n",
    "sample.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('submission.csv',index=False,header=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
